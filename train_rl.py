import pandas as pd
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

# 1. ê°•í™”í•™ìŠµ í™˜ê²½ ì •ì˜ (AIê°€ ë›°ì–´ë†€ ì„¸ìƒ)
class DynamicWeightTradingEnv(gym.Env):
    def __init__(self, df):
        super(DynamicWeightTradingEnv, self).__init__()
        self.df = df
        self.current_step = 0
        
        # [í–‰ë™ ì •ì˜] AIê°€ í•  ìˆ˜ ìˆëŠ” ì¼: 4ê°€ì§€ ì§€í‘œì— ëŒ€í•œ "ê°€ì¤‘ì¹˜(Weight)" ì •í•˜ê¸°
        # 0: ë‰´ìŠ¤, 1: RSI, 2: MACD, 3: ë³¼ë¦°ì €ë°´ë“œ
        # ê²°ê³¼ê°’: 0~1 ì‚¬ì´ì˜ ì‹¤ìˆ˜ 4ê°œ
        self.action_space = spaces.Box(low=0, high=1, shape=(4,), dtype=np.float32)
        
        # [ê´€ì°° ì •ì˜] AIê°€ ë³´ëŠ” ê²ƒ: ì •ê·œí™”ëœ ì§€í‘œ ê°’ë“¤
        # [News(0~1), RSI(0~1), MACD(-1~1), BB(0~1)]
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)

    def reset(self, seed=None, options=None):
        self.current_step = 0
        return self._next_observation(), {}

    def _next_observation(self):
        # í˜„ì¬ ë‚ ì§œì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        row = self.df.iloc[self.current_step]
        
        # ë°ì´í„° ì •ê·œí™” (AIê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ 0~1 ì‚¬ì´ë¡œ ë³€í™˜)
        obs = np.array([
            row['News_Score'] / 100.0,  # 0~100 -> 0.0~1.0
            row['RSI'] / 100.0,         # 0~100 -> 0.0~1.0
            row['MACD_Hist'],           # ê·¸ëŒ€ë¡œ ì‚¬ìš©
            row['BB_Pct']               # 0~1 (ê°€ë” ë²—ì–´ë‚˜ì§€ë§Œ ê´œì°®ìŒ)
        ], dtype=np.float32)
        return obs

    def step(self, action):
        # 1. AIê°€ ì •í•œ ë¹„ì¤‘(Action) ê°€ì ¸ì˜¤ê¸°
        # softmaxë¥¼ ì¨ì„œ ë¹„ì¤‘ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ ë§Œë“¦ (ì˜ˆ: [0.1, 0.4, 0.3, 0.2])
        weights = np.exp(action) / np.sum(np.exp(action))
        
        # 2. í˜„ì¬ ì‹œì¥ ìƒí™© ê´€ì°°
        obs = self._next_observation()
        
        # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚° (ë¹„ì¤‘ x ì§€í‘œê°’)
        # ë‰´ìŠ¤ì™€ RSIëŠ” ë†’ì„ìˆ˜ë¡ ë§¤ìˆ˜ ê´€ì , MACDë„ ë†’ì„ìˆ˜ë¡ ìƒìŠ¹, BBëŠ” ë‚®ì„ìˆ˜ë¡(í•˜ë‹¨ë°˜ë“±) ë§¤ìˆ˜
        # (ê°„ë‹¨í•œ ë¡œì§ ì˜ˆì‹œ: ê°€ì¤‘ í‰ê·  ì ìˆ˜ê°€ 0.5 ë„˜ìœ¼ë©´ ë§¤ìˆ˜)
        
        # ì§€í‘œë³„ ë§¤ìˆ˜ ì‹œê·¸ë„ ì ìˆ˜í™” (0~1)
        signal_news = obs[0] 
        signal_rsi = 1.0 - obs[1] if obs[1] > 0.7 else (obs[1] if obs[1] < 0.3 else 0.5) # ì—­ì¶”ì„¸ ì „ëµ ì˜ˆì‹œ
        signal_macd = 1.0 if obs[2] > 0 else 0.0
        signal_bb = 1.0 if obs[3] < 0.1 else (0.0 if obs[3] > 0.9 else 0.5)
        
        signals = np.array([signal_news, signal_rsi, signal_macd, signal_bb])
        
        # â˜… í•µì‹¬: AIê°€ ì •í•œ ë¹„ì¤‘ëŒ€ë¡œ ì¢…í•© ì ìˆ˜ ì‚°ì¶œ
        final_score = np.sum(weights * signals)
        
        # 4. í¬ì§€ì…˜ ê²°ì • (ì¢…í•© ì ìˆ˜ê°€ 0.6 ì´ìƒì´ë©´ ë§¤ìˆ˜)
        position = 1 if final_score > 0.6 else 0 
        
        # 5. ë³´ìƒ ê³„ì‚° (ìˆ˜ìµë¥ )
        # ë‚´ì¼ ì˜¤ë¥´ëŠ”ë° ìƒ€ìœ¼ë©´(+), ë‚´ì¼ ë‚´ë¦¬ëŠ”ë° ìƒ€ìœ¼ë©´(-)
        actual_return = self.df.iloc[self.current_step]['Next_Day_Return']
        reward = position * actual_return
        
        # 6. ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì´ë™
        self.current_step += 1
        done = self.current_step >= len(self.df) - 1
        
        # ë¡œê·¸ ì¶œë ¥ (í•™ìŠµë˜ëŠ” ê±° ë³´ë ¤ê³  100ì¼ë§ˆë‹¤ í•œ ë²ˆì”©)
        if self.current_step % 100 == 0:
            print(f"Step {self.current_step}: AI Weights = {np.round(weights, 2)} -> Reward: {reward:.2f}%")

        return obs, reward, done, False, {}

# --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == "__main__":
    # 1. ë°ì´í„° ë¡œë“œ
    try:
        df = pd.read_csv("final_rl_dataset_v2.csv")
        print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê±´")
    except:
        print("âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. process_data.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        exit()

    # 2. í™˜ê²½ ë§Œë“¤ê¸°
    env = DummyVecEnv([lambda: DynamicWeightTradingEnv(df)])

    # 3. AI ëª¨ë¸ ìƒì„± (PPO ì•Œê³ ë¦¬ì¦˜)
    print("ğŸ§  AI ëª¨ë¸ ìƒì„± ì¤‘...")
    model = PPO("MlpPolicy", env, verbose=1, learning_rate=0.001)

    # 4. í•™ìŠµ ì‹œì‘! (ê³¼ê±° ë°ì´í„°ë¥¼ ë³´ë©° ìˆ˜ì²œ ë²ˆ ì—°ìŠµ)
    print("ğŸš€ í•™ìŠµ ì‹œì‘! (ì ì‹œë§Œ ê¸°ë‹¤ë¦¬ì„¸ìš”...)")
    model.learn(total_timesteps=10000) # 10,000ë²ˆ ë°˜ë³µ í•™ìŠµ

    # 5. ëª¨ë¸ ì €ì¥
    model.save("my_trading_ai")
    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ! 'my_trading_ai.zip' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- í…ŒìŠ¤íŠ¸: í•™ìŠµëœ AIê°€ ì‹¤ì œë¡œ ì–´ë–»ê²Œ íŒë‹¨í•˜ëŠ”ì§€ ë³´ê¸° ---
    print("\n[AIì˜ íŒë‹¨ í…ŒìŠ¤íŠ¸]")
    obs = env.reset()
    for i in range(5): # 5ì¼ì¹˜ë§Œ ë³´ì—¬ì¤˜
        action, _ = model.predict(obs)
        weights = np.exp(action) / np.sum(np.exp(action)) # ë¹„ì¤‘ìœ¼ë¡œ ë³€í™˜
        print(f"ğŸ“… Day {i+1}: ë‰´ìŠ¤ë¹„ì¤‘({weights[0][0]:.2f}) vs ì°¨íŠ¸ë¹„ì¤‘({weights[0][1]:.2f})")
        # ë‹¤ìŒ ë‚ ë¡œ ì´ë™
        obs, rewards, dones, info = env.step(action)